{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd08e5ff",
   "metadata": {},
   "source": [
    "# PyTorch for Deep Learning & Machine Learning Notes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ab8db",
   "metadata": {},
   "source": [
    "### Why use machine learning (or deep learning)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815308",
   "metadata": {},
   "source": [
    "\n",
    "* Manual coding can be cumberson, machine learning can automate this.\n",
    "* Machine learing can automate complex processes with many steps that would be difficult for traditional programming.\n",
    "    * The more \"rules\" there are to a problem, the more attractive machine learning is.\n",
    "* Can adapt to continuously changing environemtns easily,  \n",
    "* ***Discovering insights within large collections of data*** (AI IS FLOURISHING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173295e6",
   "metadata": {},
   "source": [
    "### When not to use machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a37fa2",
   "metadata": {},
   "source": [
    "\n",
    "* Googles #1 rule for machine learning, \"if you can build a simple rule-based system that doesnt require machine learning, do that\".\n",
    "    * A short and simple code should be priortized for menial tasks, not complex machine learning.    \n",
    "* When you need explainability, deep learning models are uninterpretable by humans.\n",
    "* when errors are unacceptable, machine learning systems are unpredictable and not deterministic.\n",
    "* when you dont have much data, machine learning models typically require large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a9cdb",
   "metadata": {},
   "source": [
    "### When to use machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d5769",
   "metadata": {},
   "source": [
    "\n",
    "* When you have large amounts of STRUCTURED data. (rows x collumns) i.e algorithms such as XGBoost\n",
    "* Some algorithms include\n",
    "    * Random forest.\n",
    "    * gradient boosted machine.\n",
    "    * naive bayes.\n",
    "    * nearest neighbor.\n",
    "    * support vector machine.\n",
    "    * many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068ed21",
   "metadata": {},
   "source": [
    "### When to use deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b8847",
   "metadata": {},
   "source": [
    "\n",
    "* When you have large amounts of UNSTRUCTURED data (natural languages, images data) i.e neural network algorithms\n",
    "* Some algorithms include.\n",
    "    * neural networks\n",
    "    * fully connected neural networks\n",
    "    * convulutional neural network.\n",
    "    * transformers.\n",
    "    * many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f05716",
   "metadata": {},
   "source": [
    "### What are neural networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04a40a",
   "metadata": {},
   "source": [
    "\n",
    "* Inputs (images/audio/random languages) -> numerical encoding (matrixes or tensors) -> learns representation (patterns/features/weights) -> representation outputs (large numerical data set or more tensors) -> human output (image of cat/\"Hey Siri...\"/language a joke or serious?)\n",
    "    * Use the appropriate neural network for your problem (C.N.N for images or transformers for languages/audio)\n",
    "* Anatomy of Neural Networks.\n",
    "    * Input layer -> hidden layer(s) -> output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ab680",
   "metadata": {},
   "source": [
    "### Types of learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7b163",
   "metadata": {},
   "source": [
    "\n",
    "* Supervised Learning.\n",
    "    * Lots of data and examples of what the inputs should be. i.e photos of cats and dogs with each labled (data and labels)\n",
    "* Unsupervised & Self-supervised Learning. \n",
    "    * Learns solely on the data given i.e only the photos of cats and dogs, finds patterns between the two but cannot necesserily differentiate\n",
    "* Transfer Learning.\n",
    "    * Learns what one model learns and tranfers it to another. i.e can learn what Supervised Learning model discovered and transfer to another, giving a \"head start\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff455721",
   "metadata": {},
   "source": [
    "### What is deep learning used for?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf0817",
   "metadata": {},
   "source": [
    "\n",
    "* Youtube algorithms, google translate, speech recognition, computer vision, natural language processing (NLP)\n",
    "    * Translation and Speech Recognition == Sequence to Sequence (seq2seq) i.e sequence of letters or sequence of audio waves\n",
    "    * Computer Vision and Natural Language Processing (NLP) == Classification/Regression i.e Regression is predicting a number or predicting the coordinates of computer visions \"attnetion\" and\n",
    "    * Classification is predicting if something is one thing or another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fba597",
   "metadata": {},
   "source": [
    "### What is PyTorch?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadad11b",
   "metadata": {},
   "source": [
    "* Most popular research deep learning framework*\n",
    "* Write fast deep learning code in Python (able to run on a GPU(s))\n",
    "* Able to access many pre-built deep learning models (Torch Hub/torchvision.models)\n",
    "* Whole stack: preprocess data, model data, deploy model in your application/cloud\n",
    "* Originally designes and used in-house by Facebook/Meta (now open-source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665518ed",
   "metadata": {},
   "source": [
    "### What is a Tensor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d859c",
   "metadata": {},
   "source": [
    "\n",
    "* https://www.youtube.com/watch?v=f5liqUk0ZTw "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18b792",
   "metadata": {},
   "source": [
    "# What is a Classification problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7ead2",
   "metadata": {},
   "source": [
    "* **Binary Classification** - Determining whether an email is spam or not spam. Either spam or not spam, 0 or 1.\n",
    "\n",
    "* **Multiclass Classification** - More than one thing or another. Cat, dog, or bunny.\n",
    "\n",
    "* **Multilabel Classification** - Multiple lables per sample. *Grey* *Female* *longhair* Cat, *brown* *50lbs* Dog, *white* Bunny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e144616",
   "metadata": {},
   "source": [
    "Classification Inputs & Outputs\n",
    "* Inputs\n",
    "    * Photos can be broken down into 3 inputs per pixel; Width, Height, & C (color channels: R, G, B)\n",
    "\n",
    "* Outputs\n",
    "    *  Cat, Dog, Bunny [[0.97, 0.00, 0.03]] w/ each value being how certain the model is as it approaches 1.0 (e.g. This model being 97% certain of Cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e8e2f",
   "metadata": {},
   "source": [
    "Input & Output Shapes\n",
    "* Each photo input gets represented as a Tensor, `shape = [batch_size, color_channels, width, height]`, or `shape = [None, 3, 224, 224]`.\n",
    "    * batch_size - How many photos the model looks at at once. Higher batch_size, more computing power.\n",
    "    * Above Multiclass Classification input of Cat, Dog, & Bunny would result in a `Shape` output of `[3]`, while a Binary Classification would result in a `shape` of `[2]` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03916931",
   "metadata": {},
   "source": [
    "Architecture of A Classification Model.\n",
    "* Hyperparameter - Binary Classification\n",
    "    * Input layer shape (in_features) - Same as number of featers (e.g 5 for age, sex, height, smoking status) (in numerical value, e.g. male=0 female=1)\n",
    "    * Hidden layer(s) - Problem specific, minimum = 1, maximum = unlimites.\n",
    "    * Neurons per hidden layer - problem specific, generally 10 to 512.\n",
    "    * Output layer shape (out_features) - 1 shape (one class or the other) (e.g. 3 for cat, dog, or bunny)\n",
    "    * Hidden layer activation - usually `ReLU` (Rectified Linear Unit) but can be many more.\n",
    "    * Output activation - Sigmoid (`torch.sigmoid` in PyTorch).\n",
    "    * Loss function - Binary Crossentropy (`torch.nn.BCELoss` in PyTorch).\n",
    "    *Optimizer - `.SGD` (Stochastic Gradient Descent), `.Adam`, or many more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ae670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'777'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5e9bb0",
   "metadata": {},
   "source": [
    "## What goes on in a training loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d949f2",
   "metadata": {},
   "source": [
    "### What is gradient descent?\n",
    "\n",
    "Gradient descent in essence is the decrease of a the cost/loss to a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5cf74",
   "metadata": {},
   "source": [
    "### What is backpropogation?\n",
    "\n",
    "Backpropogation is the algorithm which minimizes the errors in a nn models prediction. It does this by taking the models predicted outputs, and compares them against the specified training data using a loss function algorithm (think ADAM or SGD)\n",
    "\n",
    "Backpropogation is the algorithm for determining how a single training example would like to nudge the `weights` & `bias`. **NOT** just what gives the desired effect, but what relative proportions to the changes, cause the most rapid decrease to the **total cost/loss**.\n",
    "\n",
    "* **Cost/Loss** - (outputs of the nn - desired outputs)^2 added for each piece of training data. \n",
    "\n",
    "* **Total cost/loss** - the average of cost/loss of ALL training examples\n",
    "\n",
    "* The nn outputs a hyperparametered or specified number of `out_feautures` in the model.\n",
    "\n",
    "* Each output has is given a \"probability\" of correctness, which the loss function compares to the training data. The loss function adjusts the weights & bias to nudge the outputs in the desired direction, as\n",
    "we want the predicted output to match the training data (e.g 1.0 for desired answer & 0.0 for undesired)\n",
    "    * We want the size of the adjustment to be proportional to the \"distance\" from the desired value.\n",
    "\n",
    "* focusing on a single feature/neuron, it is defined as a *weighted* sum of all the activations in the previous layer + `bias`, which is inputted into a function such as `sigmoid()` or `ReLU()`\n",
    "    * 3 avenues here to increase the chances of the desired output: Increase `bias` (how often its activated), increase `weight` (how strongly its activated), or change the activations.\n",
    "        * Increasing the `bias` is simple as it is a single independent value.\n",
    "\n",
    "        * Altering each `weight` from a single neuron/feature would be slightly different, we want to change the `weight` of the feature/neuron which will have the greatest effect, **NOT** all neurons which have any effect for efficency.\n",
    "            * \"*Neurons which activate when they see a desired value, are more likely to activate when they think of said value*\" - 3Blue1Brown.\n",
    "        \n",
    "        * We could also change all activations of features/neurons in previous layer (again, **NOT** all neurons which have any effect, but those with the **GREATEST** effect). If each neuron we want activated is activated more, & each neuron we dont want activated is activated less, then its a win.\n",
    "    \n",
    "    * These changes are all relative to any other `out_feature` that may be present which has their own wants in regards to `weight` & `bias` changes, which may also are relative the wants in regards to the *n* number of data points in the training data.\n",
    "\n",
    "* **Backpropogation** comes into play here, running through this process *backwards* recursively in each previous layer. This is whole process is then repeated for each data point in the training data.\n",
    "\n",
    "* Stochastic Gradient Descent (`SGD`) does this by randomly organizing all the data into mini-batches, then computes a step for a mini-batch (with **backpropogation**). This is less accurate than a True step through all the training data at once, but many times more time efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
